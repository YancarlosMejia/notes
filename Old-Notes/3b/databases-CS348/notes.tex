\documentclass{article}
\usepackage{parskip}
\usepackage[margin=.6in]{geometry}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}



\begin{document}
\subsection*{Compotentes}
\paragraph{Data}
Logical view and physical view of data. Logical view is what is seen by the programmers of the application and the physical view is what is seen by the system

\paragraph{People}
The users are the ones who pay for the system, we need to assume that they know nothing about the system (end user).

Administator is the one who charge of the database.

\section*{1.4 Procedure}
We have large complex data models, but we need a way to populate it with data (need to implement).

We want to reduce the dependencies between programs and data. If we change the data it should have minimal effect on the program (UI excepted). Similarly we want very little change to data when the program is changed. This cannot be eliminated, but reduced.

Three levels (external, conceptual, internal) that each have their own schemas, called the three shema approach
\paragraph{external schema} view for users

\paragraph{conceptual schema} integrates external schema

\paragraph{internal schema} defined physical storage
Three schema approach: \url{https://en.wikipedia.org/wiki/Three_schema_approach}.

When we change the application program only the external schema should change to fit it, maybe the conceptual a bit, but not the internal schema

\subsection*{Language facilities}
\paragraph{Data Definition Language (DDL)} - specifies conceptual schema and subschemas and the mappings between them.
\paragraph{Data dictionary, data directory or system catalog} - result of compilation of DDL statements in a schema; metadata.
\paragraph{Data manipulation language (DML)} - commands issued in a host program and preprocess or compiler interprets these commands as calls to DBMS. (SQL is default)
\paragraph{Query language} – a complete language for manipulating data interactively. Should be user friendly but not super powerful (interactive SQL is default)

\subsection*{Data Models}
Value based model requires an attribute that can uniquly identify an object, Object based model does not.

\subsection*{Database Design Process}
Identify functional requirements. From there figure out what data is required for those requirements, called database requirements. Structure this data in a understandable way (make a conceptual schema). This is where we make a entity relationship model. We use entity relationship models in the conceptual and logical (internal) schemas.

\subsection*{Entity Relationship Model}
Invented in 70s and grew in popularity as variations were created, so we have a family of entity relationship models. These can get complicated and hard to read (we use a specific form of notation). An entity is something that has independent existence that is relevant to application. A single value attribute (for example a telephone) has only one value. Multivalue attributes are attributes that might have multiple parts to them (like an address). Some attributes don't belong to anyone and instead describe the relationships between them. Enities in a relationship type model dont have to have a primary key to be used.

\begin{itemize}
    \item rectangle - entity type
    \item diamond - relationship type
    \item circles - attributes
\end{itemize}

Binary relationship is between two entity types.

\subsection*{Aggregation} % (fold)
\label{sub:aggregation}
Aggregation is the grouping of entities that have a connected relationship and only one connection outside that group to the rest of the entity map we can redraw the map with those entities grouped into one entity
% subsection aggregation (end)

\subsection*{Clustering} % (fold)
\label{sub:clustering}
Store chuncks to data together to make accessing it faster. Try to store a bunch in memory buffer to limit the number of times we need to read.
% subsection clustering (end)


\subsection*{Ordered Files} % (fold)
\label{sub:ordered_files}
These make searching for specific files much easier (using a binary search since their order is maintained). Whenever you change the main data file all its elements need to be indexed.

\paragraph{B+ tree} store the largest index of a block in its parent. There are three techniques, merging, splitting, and distribution. Everything is based on primary key value. The data block is represented by linked pages. Basically store the min and max values of each block in its parent block and you can just run a binary search (note: data must be sorted).
% subsection ordered_files (end)

\subsection*{Relation Scheme} % (fold)
\label{sub:relation_scheme}
See the big ass example in lecture notes, page 85.
% subsection relation_scheme (end)

\section*{Relational Algebra} % (fold)
\label{sec:relational_algebra}
Relational tables require all rows to be unique as designated by the primary key (you must have at least one candidate key that is not updated or null). Foreign keys can be used to reference rows in other tables.

\begin{verbatim}
table_constraint:=
    [CONSTRAINT name]
    {UNIQUE (<column name> + ) |
    PRIMARY KEY (<column name> + ) |
    FOREIGN KEY (<column name> + )
    REFERENCES <table name> [ON {UPDATE |
    DELETE} <effect>]}
<effect> := SET NULL | NO ACTION
    (RESTRICT) | CASCADE | SET DEFAULT
\end{verbatim}

\paragraph{Union operation} % (fold)
\label{par:union_operation}
This operation combines two tables if they have the same attributes (all the same columns) and deletes any duplicates. Denoted by $\cup$.
% paragraph union_operation (end)
\paragraph{Set Difference} % (fold)
\label{par:set_difference}
This operation returns any elements in one table that are not in  the other (tables must have same attributes). R-S results in values that are in R and not in S. Denoted by $-$.
% paragraph set_difference (end)
\paragraph{Projection} % (fold)
\label{par:projection}
This operation removes or rearranges columns. R(A,B,C) $\rightarrow$ $\pi_{C,A}(R) = R(C,A)$. Denoted by $\pi_{columns}(TABLE)$.
% paragraph projection (end)
\paragraph{Selection} % (fold)
\label{par:selection}
This operations returns rows in a table that satisfy conditions. Denoted by $\sigma_{conditions}$.
% paragraph selection (end)
\paragraph{Cartesian Product} % (fold)
\label{par:cartesian_product}
This operation combines two tables by taking their Cartesian product (make all possible combinations of rows). (A, B) X (C D, E F) = (A C D, A E F, B C D, B E F). Denoted by $\times$.
% paragraph cartesian_product (end)
\paragraph{Rename} % (fold)
\label{par:rename}
This operation renames columns. Denoted $\rho_{table(columns)}(TABLE)$.
% paragraph rename (end)
\paragraph{Natural Join} % (fold)
\label{par:natural_join}
This joins two tables based on a shared attribute. Think of a cartesian product but only for rows that have matching attributes. Denoted by $*$.
% paragraph natural_join (end)
\paragraph{Binary Relational} % (fold)
\label{par:binary_relational}
This operation is a sub operation on natural joins that provides a condition to the join to only include rows that satisfy the condition. This is equivalent to running the set operator on a cartesian product. $R*S_{\theta condition} := \sigma(R\times S)$. Denoted $\sigma$.
% paragraph binary_relational (end)
\paragraph{Intersection} % (fold)
\label{par:intersection}
This operation returns any rows that are in both tables (tables must have matching attributes). Denoted $\cap$.
% paragraph intersection (end)
% section relational_algebra (end)

\section*{Views} % (fold)
\label{sec:views}
Views are just a virtual table that compute dynamically.

To create a view:
\begin{verbatim}
    CREATE VIEW <view name> (<list of column names>)
        AS <subquery> <with check option>
\end{verbatim}
Where the subquery is the selector of what rows are in the view (views are subsets of tables). The with check option says that any operations to the view that would be invisible to the view (ie. not fit the selector for the view) wouldn't be allowed.

To delete a view:
\begin{verbatim}
    DROP VIEW <view name>
\end{verbatim}

When updating a view you run into a problem if the view's data is representative of a calculation done on the underlying table (imaging a column of sums). You cannot update this data because it would be inaccurate as updating a view doesn't touch the underlying table. There is no good way to do this aside from putting in restrictions on what you can an cannot update.
% section views (end)
\section*{Embedded SQL} % (fold)
\label{sec:sql_basics}
\textbf{Intearctive SQL}: statements are input from the terminal

\textbf{Non-inteactive SQL}: compiled in the application language

\textbf{Statement Level Interface}: application mixes in sql statements to their host language (usually have special syntax to tell sql from host language), the precompiler finds sql and translates it into calls to the host language which is then compiled by the host language compiler and executed. These can be static or dynamic sql.

\textbf{Call Level Interface}: sql statements are passed around as strings that are executed by the host language
% section sql_basics (end)

\section*{Static SQL} % (fold)
\label{sec:static_sql}
VARCHAR: oracle data type is how we get variables. Variables that are referenced by sql statements are \textbf{host variables} and prefixed with a colon. These must be defined within an embeded sql declaration section
% section static_sql (end)

\subsection*{Cursor Operations} % (fold)
\label{sec:cursor_operations}
A cursor is created by EXEC SQL DECLARE <curson name>. INSENSITIVE option creates a copy of the rows in the result set and all accesses through the cursor will be to that copy. If this is not specified then we are working with the actual row and can edit it (unless read only is declared). OPEN operation executes the query related to the cursor and we can use a couple of operations to advance it (FIRST, NEXT, PRIOR, etc). We need to specify SCROLL when declaring the cursor to use these operators, else only NEXT is allowed. We then give the cursor declaration a FOR

% subsection* cursor_operations (end)

\subsection*{Update Operation} % (fold)
\label{sec:update_operation}
this updates the current row that the cursor is pointing at with the given values.
% subsection* update_operation (end)

Some cursor update sample code
\begin{verbatim}

\end{verbatim}
exec sql declare cursor c for ...;
exec sql open cursor c;
while (true){
    exec sql fetch c into :var ...;
    exec sql update <table> set <attribute> = <expr>
    where current of c;
    exec sql commit work;} //commit or rollback signals the end of work

\section*{Dynamic SQL} % (fold)
\label{sec:dynamic_sql}
This allows you to execute commands where you dont know the full context at the time. May have slower performance as things are stored in strings then prepared and executed instead of straight execution. we many even use dummy host variables which dont need to be declared.

If you want to execute immediately you can use EXECUTE IMMEDIATE <variable the statement is stored in>. Else you prepare by processing into static sql using the PREPARE <statement name> FROM <statement command>, then we execute by passing in a parameter list that will substitute for dummy values using the EXECUTE <statement name> [INTO <variable list>][USING <parameter list>].

% section dynamic_sql (end)

Here we use cursors the same way by declaring them with OPEN.

\section*{JDBC} % (fold)
\label{sec:jdbc}
This is a CLI for executing SQL in a Java program. The statement is constructed at run time as a java variable which is then passed to an underlying dbms through a driver.

We connect to the data based, send statements, then process the results. See notes for sample code.

Connect to database with DriverManager.getConnection(url, id, password) if successfull it creates a connection object (called con from now on)

Create a statment object with con.createStatement(), these have executeQuery and executeUpdate abilities. Called stat.

We then assign a query string (just a string of sql) to with through stat.executeQuery(string) which returns a set of of results. Query string can have variables in it as it is constructed at run time.

We prepare the query string used above with con.prepareStatement(string). We mark placeholders with ? where actual values are plugged in using the preparedStatement's setString function (takes a index and value to put there). We can then run the executeQuery function on thsi prepared string.

We can get data from a column using getTYPE function
\begin{verbatim}
while ( res.next ( ) ) { // advance the cursor
    j = res.getString (“COF_NAME”); // fetch output value
    ...process output value...
}

could also have passed in the column index.
\end{verbatim}
% section jdbc (end)

\section*{Schema Analysis} % (fold)
\label{sec:schema_analysis}
This is where shit gets weird.

Notation
\begin{itemize}
    \item R, X, Y, Z - sets of attributes or relation schemes.
    \item r - a relation defined on a relation scheme R.
    \item s, t, u, v - tuples defined on some relation scheme.
    \item A, B, C, D, E - single attributes.
    \item ABC - a set consists of the attributes A, B and C.
    \item F, G, H - sets of functional dependencies.
    \item f, g, h - single functional dependencies.
    \item Let t be a tuple. Then t[X] denotes the X components of t.
\end{itemize}

\subsection*{Functional Dependencies} % (fold)
These are denoted as fd's. For example a relationship r \textbf{satisfies} $X\rightarrow Y$ if whenever two tuples in r agree on their X components they also agree on their Y components. Basically it denotes instances where the values in one column are dependant on another (for example all cars in the canada are tax free so the tax column relies on the country column).

$X\rightarrow Y$ is \textbf{trivial} if Y is a subset of X.

A fd g is \textbf{implied} by f (denoted by F |= g) if whenever r satisfies F it satisfies g.

The \textbf{closure} of a set of fd's F, $F^+ = \{X\rightarrow Y | F |= X \rightarrow Y\}$. This means that the closure of F ($F^+$) is the set of all dependencies such that F logically implies those dependencies

A fd's F is a \textbf{cover} (meaning equal to) of G (denoted $\approx$) if $F^+ = G^+$. This means that two sets of dependencies are considered equal to each other if they have the same closure (logically imply the same set of dependencies).

The \textbf{closuer} of a set of attributes X is $X^+ = \{A | X\rightarrow A is in F^+\}$. This is the set of all attributes that have a dependency to the chosen attribute (X) as defined by some dependency closure($F^+$).

X is a \textbf{candidate key} of R if $X\rightarrow R\in F^+$ and no proper subset of X has this property. Translated, for a set of attributes R, an set of attributes X is a candidate key of that set if the dependency of X to R exists in the given closure of dependencies $F^+$ and no subset of X can fulfill this property (candidate key must be minimal).

Y is a \textbf{superkey} of R if Y contains a key of R (where Y and R are sets of attributes), basically every relation is dependent on this set of attributes.

\textbf{Theorem}: Given F, XY is a subset of R then:
\begin{itemize}
    \item $F|= XY$
    \item Y is a subset of $X^+$
\end{itemize}
This means that for a set of dependencies F and two sets of attributes X and Y that are a subset of a larger set of attributes R we can say that if F logically implies that X has a dependency Y then Y is a subset of the attribute cover of X, $X^+$, and vice versa.

\textbf{Theorem}: Let $Y=A_1 ... A_n$ then $F|=X\rightarrow Y$ iff $F|=X\rightarrow A)n$ and ... $F|=X\rightarrow A_n$
This means that for a set of n attributes Y, we can only say F logically implies the dependency of X to Y if it implies the dependencies of X to each attribute in Y.

F is a \textbf{minimal conver} if
\begin{itemize}
    \item every right side of a dependency in F is a single attribute
    \item there exists no $X\rightarrow A$ in F such that $F-\{X\rightarrow A\}\approx F$ (if you removed the dependency F would cover itself, ie no dependency is negligible)
    \item there exists no $X\rightarrow A$ in F and a proper Z of X such that $F-\{X\rightarrow A\}\cup\{Z\rightarrow A\}\approx F$ (there is no subset of X that if its dependencies were removed F would cover itself, ie no subset of X is negligible)
\end{itemize}

\textbf{How to compute closer}: given a set of dependencies F and a set of attributes X. First define the closure set to equal X so that it is nonempty. For each dependency in F, if the lhs is a subset of the current closure set and the rhs is not in the closure set then add the rhs to the closure set. At the end of this loop you will have computed the closure for X ($X^+$).

% section schema_analysis (end)

\section*{Decomposition} % (fold)
\label{sec:decomposition}
We can break up a large table into a series of smaller tables by creating a new table for each dependency in the main table.

An attribute A in a set of attributes R is \textbf{prime} if it is in X for some candidate key X. Basically a prime attribute is an attribute in a candidate key for a set of dependencies R (think of R as a table).

\section*{Normal Form} % (fold)
\label{sec:normal_form}
\textbf{First Normal Form}: denoted 1NF, attributes are atomic meaning they have no internal fields (at least none that the relations and schemas care about), an example of a violation of this is if you had a relation between parent and a list child (we cannot query individual children in this relation), to solve this we create an entry for each parent child relation (multiple entries for each parent)

\textbf{Second Normal Form}: denoted 2NF, this requires it to be 1NF and have any non-prime attributes depend on a candidate key (all of the key, not a subset), an example of a violation would be if the example solution to 1NF incorporated the gender of parents so that the parent to child relation was a candidate key (since this is unique) and the gender of parent is non-prime (cannot form a candidate key using it and anything else) but it doesn't depend on the parent-child candidate key. To fix this you need to make a new parent to gender table where this pairing works.

\textbf{Third Normal Form}: denoted 3NF, requires it to be 2NF and have no dependencies among any of the non-prime attributes, an example violation would be if the solution to 2NF example included a type of parent (father or mother) that is dependent on gender of parent, but both of these are non-prime keys (aren't part of candidate key). To fix this you take out the type of parent value from table and create a new table for gender to type relation.

\textbf{Boyce-Codd Normal Form}: denoted BCNF, created to deal with how 3NF doesn't handle overlapping candidate keys well, this requires that every instance of an attribute being fully dependent on another it is a super key.

If a table has only one candidate key then being 3NF makes it BCNF

% section normal_form (end)

\section*{Lossless Join} % (fold)
\label{sec:lossless_join}
This is the concept that you should not lose any information in the decomposition of a scheme. We do this so that when we natural join the decomposition the attributes involved in the join are candidate keys for the relationships associated with the initial table. This is done to prevent generating new rows in the joined table that weren't in the original. For example R=(A, B, C, D, E) with dependencies A$\rightarrow$BC, CD$\rightarrow$E, B$\rightarrow$D, and E$\rightarrow$A, and we break it into R1 = (A, B, C) and R2 = (A, D, E). This is a lossless join because we join R1 and R2 on A, and A$\rightarrow$BC which we can expand trivially into A$\rightarrow$ABC which is the same as A$\rightarrow$R1. Basically the decomposition must be joinable onto a set of attributes that can imply one of the decomposed relations based on the initial relations functional dependencies. You can test this using the \textbf{chase algorithm}.

\subsubsection*{Chase Algorithm} % (fold)
\label{ssub:chase_algorithm}
First you create a table (called tableau). There is a row for each dependancy and a column for each relation. For each cell put the name of the attribute if it is involved in the corresponding relation, else give the cell a unique marker (usually the attribute name with a counter). Then decompose all dependencies so that each has only one attribute on the right. Iterating through each dependency, try to match column, where rows have matching values on the left of a dependency they should have matching values for the right side. Try to get a row that has all unscripted values, if you can do this the algorithm terminates and you have a lossless join. The prof executes this using $\phi$ for all scripted values and a for unscripted ones and you try to convert an entire row to a's.

see \url{http://stackoverflow.com/questions/23596464/lossless-join-and-decomposition-from-functional-dependencies} for details
% subsubsection chase_algorithm (end)
% section lossless_join (end)

\section*{Dependency Preserving} % (fold)
\label{sec:dependency_preserving}
When decomposing things we need to watch for violating any dependencies. Basically list all dependencies that each relation can hold and the cover of the original relation must be a subset of the union of them.
% section dependency_preserving (end)

\section*{Example Problems} % (fold)
\label{sec:example_problems}
R(A, B, C, D, E), F = {A$\rightarrow$B, BC$\rightarrow$E, ED$\rightarrow$A}

\textbf{1}List all keys

ED$\rightarrow$A $\implies$ so by identity ED$\rightarrow$ADE $\implies$ we know that A$\rightarrow$B so ED$\rightarrow$ABDE $\implies$ then add C to get CED$\rightarrow$ABCDE $\implies$ CDE is a key.

BC$\rightarrow$E $\implies$ so by identity BC$\rightarrow$BCE $\implies$ we can add BCD$\rightarrow$BCDE $\implies$ we know that ED$\rightarrow$A so BCD$\rightarrow$ABCDE $\implies$ BCD is a key.

A$\rightarrow$B $\implies$ so by identity A$\rightarrow$AB $\implies$ we can add AC$\rightarrow$ABC $\implies$ we know that BC$\rightarrow$E so AC$\rightarrow$ABCE $\implies$ we can add D ACD$\rightarrow$ABCE $\implies$ ACD is a key.

\textbf{1NF}: yes, because no attributes are collections

\textbf{2NF}: yes, because all attributes are prime (all attributes are in one of the candidate keys)

\textbf{3NF}: yes, because all attributes are prime

\textbf{BCNF}: no, because A$\rightarrow$B but A is not a super key(since there are instances of attributes not being dependent on A)
% section example_problems (end)

\section*{Transaction Management} % (fold)
\label{sec:transaction_management}
Just like anything else the database can crash, for instance if you attempt to read data that doesn't exist yet. Concurrency really fucks with this. The \textbf{dirty read problem} is when one transaction reads data from a row that has been modified but not committed (reading stale data). \textbf{Inconsistent analysis problem} is when a transaction reads the same data multiple times and it keeps changing due to another transaction committing changes to it.

\url{https://technet.microsoft.com/en-us/library/aa213029(v=sql.80).aspx}


ACID
\begin{itemize}
    \item Atomicity - transaction performed in its entirety or not at all
    \item Consistency - keep the database consistent (dont crash it)
    \item Isolation - concurrently acting transactions shouldnt interfere with each other
    \item Durability - committed data should not be lost
\end{itemize}

The \textbf{schedule} is the order that transactions will be executed in (to monitor concurrent transactions).

A \textbf{cascading rollback} is the database rolling back all transactions that relied on a transaction that caused a failure (this can snowball quite far, hence the name cascading). A schedule avoids cascading rollback if it only reads items that were fully committed by transactions (then you only have to ever rollback on transaction).
% section transaction_management (end)

\section*{Concurrency Control} % (fold)
\label{sec:concurrency_control}
A \textbf{serial schedule} has each transaction run to completion before beginning the next. If a nonserial schedule has the same effect as a serial schedule it is \textbf{serializable}.

% section concurrency_control (end)
\section*{Serializability} % (fold)
\label{sec:serializability}
A serializable schedule has the same effect as if you finished each transaction completely before moving on. Two schedules are \textbf{view equivalent} if:
\begin{itemize}
    \item the set of transactions are the same
    \item if one reads the initial value so does the other (before it gets fucked with)
    \item the value written by one transaction will read the same for each transaction
    \item a transaction that writes the final value for each transaction will write the same value
\end{itemize}
A vies is \textbf{serializable} if is view equivalent to a serial schedule.

\url{http://www.tutorialspoint.com/dbms/dbms_transaction.htm}

\paragraph{Read before Write Model} % (fold)
\label{par:read_before_write_model}
A transaction must read data before it can be written and cannot have a read without a write. Assume that some function has been executed between the read and write.
% paragraph read_before_write_model (end)

\paragraph{Conflicts} % (fold)
\label{par:conflicts}
Two steps in a schedule are conflicting if they are in different transactions on the same data and one is a write. These might produce different effects on the database if you swapped their order of execution.
% paragraph conflicts (end)

Test for serializability:
\begin{enumerate}
    \item create a precedence graph
    \begin{enumerate}
        \item add node for each transaction
        \item add edge where two transactions contain conflicting steps from the transaction that has the first step in the conflict
    \end{enumerate}
    \item if there is a cycle it is not serializable
    \item else find an ordering via topological sort
    \begin{enumerate}
        \item start with node with no incoming edges, remove it and all outgoing edges
        \item repeat until empty
    \end{enumerate}
    \item output is now a serial schedule
\end{enumerate}

% section serializability (end)

\section*{Locking} % (fold)
\label{sec:locking}
You have RLOCK that prevents others from writing that data and WLOCK which prevents others from reading it. Call UNLOCK when done. Livelock and deadlock are problems.
\begin{itemize}
    \item Prevention
    \begin{itemize}
        \item get all locks at start
        \item order lock requests
    \end{itemize}
    \item Detection
    \begin{itemize}
        \item draw a wait for graph
        \begin{itemize}
            \item node for each transaction
            \item edge from A to B if A is waiting to lock an item held by B
            \item if a cycle exists deadlock has happened
            \item kill one transaction to fix
        \end{itemize}
    \end{itemize}
\end{itemize}

The test for serializability is exactly the same as earlier just with locks instead of actions.

\textbf{Two Phase Unlocking} is a protocol stating that all locks must precede all unlocks (lock down everything first), this ensures serializability
% paragraph two_phase_locking (end)

\subsubsection*{Isolation Levels} % (fold)
\label{ssub:isolation_levels}
In relational databases we access all items that satisfy a predicate which makes it much harder to control concurrency so we need different isolation levels. We run into the problem of \textbf{phantom updates}. This is when the order of transactions can cause two sequential reads to be different. Locking down a row in a table does not prevent an update from adding rows that would fit the predicate. To fix this you can use predicate locking. These, of course, can conflict.

Under two phase lock a transaction doesn't release until it commits or aborts. We can weaken the lock to increase concurrency. This brings about various isolation levels:
\begin{itemize}
    \item read uncommitted(lowest) - dirty reads are allowed
    \item read committed - only write locks must be held until end of transaction so that all data is committed right before a read of it, prevents dirty reads, but consecutive reads may differ
    \item repeatable read - this holds locks until end of transaction but predicate locks are not managed so phantom updates can occur
    \item serializable(highest) - this is the isolation level described above where locks are required to be released at end of transaction
\end{itemize}

\paragraph{Lock Types} % (fold)
\label{par:lock_types}
Locks can be on rows/predicates, read/write, short/medium/long. Short locks execute a statement then release and long are held until end of transaction. Write locks are handled the same at all isolation levels, long locks are for update/insert/delete. Read locks differ with level:
\begin{itemize}
     \item read uncommitted - no read locks (a transaction can read a write locked item allowing dirty reads)
     \item read committed - short duration read on rows and predicates (prevents dirty reads)
     \item repeatable read - long duration read on rows and short duration on predicate
     \item serializable - long duration read on rows and predicates
 \end{itemize}
% paragraph lock_types (end)
% subsubsection isolation_levels (end)

\paragraph{Cursor Stability} % (fold)
\label{par:cursor_stability}
This is a spin of of read committed where the read lock on a row access through a cursor is medium duration, it is held until the cursor moves
% paragraph cursor_stability (end)
% section locking (end)

\section*{Crash Recovery} % (fold)
\label{sec:crash_recovery}
Storage types:
\begin{itemize}
    \item volatile: data is lost easily
    \item nonvolatile: slightly more reliable
    \item stable: data is assumed to be permanent
\end{itemize}

Failure types:
\begin{itemize}
    \item Transaction-local: only one transaction fucked up and only one needs to be rolled back
    \item system-wide: some or all transactions need to be rolled back
    \item media: cannot recover, data lost
\end{itemize}

\textbf{Logical file}: file as seen by programmer (set of logical records)

\textbf{Block} atomic unit of storage, also called a page (may contain one or more logical records)

When you open a file a buffer the size of the file is allocated. Blocks are moved to the buffer by input or output commands. Fuck with it by read or write commands. The input/output command just take a parameter telling it what record to access, the read/write commands take a parameter describing the record to access and one for the variable containing the information you want to use.

\subsubsection{Recovery Schemes} % (fold)
\label{ssub:recovery_schemes}
\paragraph{Log based Technique} % (fold)
\label{par:log_based_technique}
keep a log file of sequential changes to the system and keep it on stable storage (for every change store a record containing the changed values in it). This way we can defer updates in the log until we can be sure it won't fuck shit up. You can also do stuff with immediate updates by letting everything through and keep a log of the state at a time and if a failure occurs and rollback if required.
% paragraph log_based_technique (end)

\paragraph{Media Failure Recovery} % (fold)
\label{par:media_failure_recovery}
Whenever no transactions are happening dump the database into stable storage incase failure.
% paragraph media_failure_recovery (end)
% subsubsection recovery_schemes (end)

\subsection{Security} % (fold)
\label{sub:security}
Security protects data against unauthorized access. Create accounts, assign/remove privilege, assign security level.

Account level privilege allos the creation of stuff, the relation level allows us to control people's privileges (via AUTHORIZE key word). Each relation in database has an owner (person who created it) who can pass privileges to other users (using GRANT or REVOKE then the list of functions they are allowed to execute ON then teh list of columns they can touch TO username).
% subsection security (end)
% section crash_recovery (end)


%stopped at page 2

\end{document}
