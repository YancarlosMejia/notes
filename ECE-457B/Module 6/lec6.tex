\documentclass{article}
\usepackage{parskip}
\usepackage{pdfpages}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[margin=.6in]{geometry}
\begin{document}
\includepdf[pages=1-6]{slides}
This is the struture of the neural network. It is the most general structure. An input layer is where signals enter the system. These are usually the features of an object. The yellow circles are just units. We call them perceptrons, this is a bit misleading. These perceptrons are different from the ones we saw earlier. The earlier ones create discrete values but they instead (most of the time) they have a smooth, nonlinear activation function. Their activation functions are very differentiable. They have the same structure though. The output layer generates the actual output of the system.

\includepdf[pages=7-9]{slides}
The multilayer perceptron has many layers to propagate signals. Most of the time, these kinds of models are supervised (we have to teach them).

This is still an optimization problem. We have some weights that we want to optimize our weights so that they have the least error. These kinds of systems have a ton of dimensions to optimize across. There will be very complex structures which make it very hard to actually solve. This is why we tend towards using other systems to get an optima even if its not the global optima.

When a solution is found (weights are known) we use this to find the weights of the next layer. This backpropagates the error signal through the layers backwards untill it reaches the first layer. You keep feeling in training signals until you run out and update all the weights. At this point we have finished one epoch.

Say you have $\overrightarrow{x}(x_1, ..., x_m)$ then you have the output $\overrightarrow{y}(y_1, ..., y_l)$ so we then get the mapping $\overrightarrow{y} = f(\overrightarrow{x})$. So your first training signal has to be a bunch of mappings of x to y. Roughly k signals.

\includepdf[pages=10]{slides}
Here $i$ is the index of the element we are looking at and $k$ is the index of the training set we are currently looking at. This makes an error function. We compute the sum of the square error for each node and propagate it backwards. We keep going backwards until we reach the communication layer (one right after the input layer).
\includepdf[pages=11]{slides}
\includepdf[pages=12]{slides}
Here is the update function for the weights. It looks pretty standard. We work layer by layer.

\includepdf[pages=1]{additional}
Here we have input layer $x$ with $m$ nodes in it, middle layer $z$ with $l$ nodes in it, and output layer $y$ with $n$ nodes in it. There are three weights going between $x$ and $z$ labeled with $v$, there are other ones but we are going to ignore them for simplicity in this example. There are other weights from $z$ to $y$ labeled as $w$, again there are others that we are just ignoring to make the example easier to go through. The subscript on the notation is the destination node,origin node notation. So how do we find the weights?

\begin{align*}
	a &= \text{activation function}\\
	net_q &= \sum_{j=1}^n v_{qj}x_j\\
	z_q &= a(net_q)\\
	&= a(\sum_{j=1}^n v_{qj}x_j)\\
	net_i &= \sum_{q=1}^l w_{iq}z_q\\
	y_i&= a(net_i)\\
	y_i &= a(\sum_{q=1}^l w_{iq}z_q)\\
	y_i &= a(\sum_{q=1}^l w_{iq}a(\sum_{j=1}^n v_{qj}x_j))\\
\end{align*}
In this case the two a functions do not have to be the same but its much easier if they are.

What about the target data (denoted $d_i$)?
\begin{align*}
	E &= \frac{1}{2}\sum_{i=1}^n(d_i - y_i)^2\\
\end{align*}

Remember we want gradient decay that eventually minimizes the error, after all this is an optimization problem.

Now we need to find our update functions for the shit ton of weights we have:
\begin{align*}
	\Delta w_{iq} &= -\eta\nabla_{w_{iq}} E(w)\\
	&= -\eta\frac{\delta E}{\delta w_{iq}}\\
	&= -\eta\frac{\delta E}{\delta y_{i}}\frac{\delta y_i}{\delta net_i} \frac{\delta net_i}{\delta w_{iq}}\\
	&= \eta(d_i - y_i)a'(net_i) z_q\\
	&= \eta\delta_{oi}z_q\\
	\delta_{oi}&= (d_i - y_i)a'(net_i)\\
	\Delta v_{qj} &= -\eta\nabla_{v_{qj}} E(w)\\
	&= -\eta\frac{\delta E}{\delta v_{qj}}\\
	&= -\eta\frac{\delta E}{\delta net_q}\frac{\delta net_q}{\delta v_{qj}}\\
	&= -\eta\frac{\delta E}{\delta z_q}\frac{\delta z_q}{\delta net_q}\frac{\delta net_q}{\delta v_{qj}}\\
	&= \eta\sum_{i=1}^{n}\left ((d_i - y_i)a''(net_i)w_{iq} \right )a'(net_q)x_j\\
	&= \eta \delta_{hq}x_j\\
	\delta_{hq} &= a'(net_q) \sum_{i=1}^n \delta_{oi}w_{iq}
\end{align*}
This is the output for only one hidden layer, more work is required for more layers.

We snag E and keep it constant though all the layers and let it run until E updates with a new training signal. So the E value should be the same through out all the udpates.

To see the full slide of this:

\includepdf[pages=2-4]{additional}

\includepdf[pages=5-6]{additional}
Now lets say we have an arbitrary number of layers $Q$, so the $z$ layer in the previous example becomes ${}^{q-1}y$ and the $y$ layer becomes ${}^qy$.

The tolerance error is the max error we will allow. We know it will never reach 0 so we instead give it a threshold to work for. Something that is good enough.

\includepdf[pages=7]{additional}
Step 1: apply your first set of data. Starting at the top layer (where $q+1$).

Step 2: propagate the signal forward. Keep calculating the output  of each layer until  you get to the end.

Step 3: calculate the error of the system. The error equation shows two Es. This one being added at the time is the previous error, so its cumulative.

\includepdf[pages=8]{additional}
Step 4: propagate the error signal back. Here we are just updating our weights as we go.

Step 5: just cycle through a nice epoch

Step 6: check if your error is low enough. cycle if not

steps 5 and 6 are basically the same looping conditions of the adeline system.

\includepdf[pages=18-19]{slides}

\includepdf[pages=11-17]{slides}
$\Delta w^L = \delta^L x^{L-1}$ This is the relationship between layers. These will propagate backwards untill it reaches the input.

\begin{align*}
	\Delta w_{ij}^L &= \eta \left[ t_i - o_i^L \right] f'(tot)_i^L o_J^{L-1}\\
	f\(tot)_i^L = \frac{\deltaf(tot)^L}{\delta tot_i^L}\\
	\delta_i^L &= \text{Error of the system}\\
\end{align*}

Here $f$ is the sigmoid function $f(x) = \frac{1}{1+e^{-x}}$. We like this activation function because it is infinitely differentiable $\frac{\delta f}{\delta x} = f(1-f)$. This is almost universally used.

\begin{align*}
	\Delta w_{ig}^L &= \eta \delta_i^L o_j^{L-1}\\
	\delta_i^L &= (t_i - o_i^L)(o_i^L)(1-o_i^L)\\
\end{align*}


All of the above processes are online training. An alternative is batch/offline training where you put all of the input in at the same time. Usually online returns better results but there are places where offline training is helpful.

Now we look at propagating backwards: $l < L$
\begin{align*}
	\Delta w_{ij}^l &= \eta \delta^lo_j^{l-1}\\
	\delta_i^l &= f'(tot_i)^l \sum_{p=1}^{nl} \delta_p^{l+1}w_{pi}^{l+1}\\
	nl &= \text{number of hidden layers}\\
	\delta_i^l &= o_i^l(1-o_i^l)\sum_{p=1}^{nl} \delta_p^{l+1}w_{pi}^{l+1}\\
	\Delta w_{ij}^l &= \eta o_i^l(1-o_i^l) \sum_{p=1}^{nl} \delta_p^{l+1}w_{pi}^{l+1}
\end{align*}
These equations areht emost equations used in back propagation.

There are some typos:
\begin{itemize}
	\item slide 29, lowest line should be $\delta_6 = $ the rest of the line
\end{itemize}











\end{document}
