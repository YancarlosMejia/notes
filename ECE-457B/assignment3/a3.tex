\documentclass[12pt,a4paper]{article}
\usepackage{parskip}
\usepackage{pdfpages}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\usepackage[margin=.6in]{geometry}

\usepgfplotslibrary{fillbetween}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bofttom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}

\begin{document}
\section*{Question 1}
\label{sec:question_1}
First calculate y:
\begin{align*}
	y &= a(w_{13}z_3 + w_{14}z_4)\\
	&= a(w_{13}(v_{31}z_1 +v_{32}z_2) + w_{14}(v_{41}z_1 +v_{42}z_2))\\
	&= a(w_{13}(v_{31}(u_{11}x_1+u_{12}x_2) +v_{32}(u_{21}x_1+u_{22}x_2)) + w_{14}(v_{41}(u_{11}x_1+u_{12}x_2) +v_{42}(u_{21}x_1+u_{22}x_2))\\
\end{align*}

Then calculate the error:
\begin{align*}
	E &= \frac{1}{2}(d-y)^2 + E_{prev}\\
	\delta &= (d-y)a'(net_i)\\
	 &= (d-y)\alpha sech^2(\alpha w_{13}z_3 + \alpha w_{14}z_4)\\
\end{align*}

Step 4: propagate error signal backwards
\begin{align*}
	\Delta w_{13} &= -\eta \nabla w_{13}E(w)\\
	&= -\eta \frac{\delta E}{\delta y} \frac{\delta y}{\delta net_{y}} \frac{\delta net_{y}}{\delta w_{13}}\\
	&= -\eta(d-y)a'(w_{13}z_3+w_{14}z_4)z_3\\
	&= -\eta(d-y)\alpha sech^2(\alpha w_{13}z_3+\alpha w_{14}z_4)z_3\\
	\Delta w_{14}&= -\eta(d-y)\alpha sech^2(\alpha w_{13}z_3+\alpha w_{14}z_4)z_4\\
	\Delta v_{31} &= -\eta \nabla v_{31}E(w)\\
	&= -\eta \frac{\delta E}{\delta y}\frac{\delta y}{\delta z_3}\frac{\delta z_3}{\delta net_{z_3}}\frac{\delta net_{z_3}}{\delta v_{31}}\\
	&= -\eta \frac{\delta E}{\delta y}\frac{\delta y}{\delta z_3}\frac{\delta z_3}{\delta net_{z_3}}z_1\\
	&= -\eta \frac{\delta E}{\delta y}\frac{\delta y}{\delta z_3}a'(v_{31}z_1 + v_{32}z_2)z_1\\
	&= -\eta \frac{\delta E}{\delta y}a'(w_{13}z_3 + w_{14}z_4)(w_{13}v_{31})a'(v_{31}z_1 + v_{32}z_2)z_1\\
	&= -\eta (d-y)a'(y)a'(w_{13}z_3 + w_{14}z_4)(w_{13}v_{31})a'(v_{31}z_1 + v_{32}z_2)z_1\\
	\Delta v_{32} &= -\eta (d-y)a'(y)a'(w_{13}z_3 + w_{14}z_4)(w_{13}v_{32})a'(v_{43}z_1 + v_{42}z_2)z_1\\
	\Delta v_{41} &= -\eta (d-y)a'(y)a'(w_{13}z_3 + w_{14}z_4)(w_{14}v_{41})a'(v_{41}z_1 + v_{42}z_2)z_1\\
	\Delta v_{42} &= -\eta (d-y)a'(y)a'(w_{13}z_3 + w_{14}z_4)(w_{14}v_{42})a'(v_{41}z_1 + v_{42}z_2)z_2\\
\end{align*}

Using the above equations propagate the error signal backwards and update the weights as you go.

\section*{Question 2}
\label{sec:question_2}
\subsubsection*{a)}
\label{ssub:a_}
First Equation:\\
\begin{tabular}{|c|c|}
\hline
\textbf{number of samples} & \textbf{average error}\\
	\hline
	10 & 0.545115141138\\
	\hline
	20 & 0.245958005301\\
	\hline
	30 & 0.286637438925\\
	\hline
	40 & 0.3207765774\\
	\hline
	50 & 0.192286288873\\
	\hline
	60 & 0.209239831276\\
	\hline
	70 & 0.243411233516\\
	\hline
	80 & 0.163824200894\\
	\hline
	90 & 0.3772575898\\
	\hline
	100 & 0.346558296457\\
	\hline
	110 & 0.249353776456\\
	\hline
	120 & 0.279108234307\\
	\hline
	130 & 0.287499210734\\
	\hline
	140 & 0.370061850401\\
	\hline
	150 & 0.442842574873\\
	\hline
	160 & 0.199451224358\\
	\hline
	170 & 0.218001441535\\
	\hline
	180 & 0.301192509787\\
	\hline
	190 & 0.217476980488\\
	\hline
\end{tabular}

It appears that the average error tended to decrease as the number of samples increased until around 50 samples at which point the error starts to increase, probably as the result of overfitting.

Second equation:

\begin{tabular}{|c|c|}
\hline
\textbf{number of samples} & \textbf{average error}\\
\hline
	10 & 0.0449312608146\\
	\hline
	20 & 0.109177596847\\
	\hline
	30 & 0.109312547095\\
	\hline
	40 & 0.183494780438\\
	\hline
	50 & 0.185883223375\\
	\hline
	60 & 0.0885110338373\\
	\hline
	70 & 0.171174321787\\
	\hline
	80 & 0.0986018793229\\
	\hline
	90 & 0.0928874629844\\
	\hline
	100 & 0.208938778366\\
	\hline
	110 & 0.109345307753\\
	\hline
	120 & 0.18107348994\\
	\hline
	130 & 0.231213524392\\
	\hline
	140 & 0.165746310569\\
	\hline
	150 & 0.153850856933\\
	\hline
	160 & 0.120771236064\\
	\hline
	170 & 0.135302449973\\
	\hline
	180 & 0.154132737489\\
	\hline
	190 & 0.119314927447\\
	\hline
\end{tabular}

It appears that the average error tended to decrease as the number of samples increased until around 100 samples at which point the error starts to increase, probably as the result of overfitting.

\subsubsection*{b)}
\label{ssub:b_}
For the first equation, when evaluated over 100 data samples and within 1-100 hidden nodes the best error was found at 56 nodes. The data seems to cycle the error values seemed to cycle.

For the second equation, when evaluated over 100 data samples and within 1-100 hidden nodes the best error was found at 11 nodes. The data seems to cycle the error values seemed to cycle.

\subsubsection*{c)}
\label{ssub:c_}
It appears that the error values oscillate cyclically, there is a probably an optimal value to be found before the network starts to get too finely tuned.

\section*{Question 3}
\label{sec:question_3}
The number of nodes

\begin{tabular}{|c|c|}
\hline
2 & 0.154132737489\\
\hline
6 & 0.109345307753\\
\hline
8 & 0.0928874629844\\
\hline
12 & 0.183494780438\\
\hline
20 & 0.231213524392\\
\hline
\end{tabular}

The performance of the system seems to peak around 8 hidden neurons. This seems to imply that there the number of neurons can inversely effect the accuracy of the system if there are too many. This could have the same effect as overfitting.








\end{document}
